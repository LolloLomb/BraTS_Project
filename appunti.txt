Tipo di immagini:
    T1, inutile, non la useremo
    T1CE, T1 con mezzo di contrasto evidenzia le aree tumorali attive poichè il contrasto si accumula nelle zone maggiormente danneggiate dal tumore.
    T2, permette di vedere i liquidi, quindi le zone interessate da edèmi e infiammazioni (che sono reazioni biologiche)
    FLAIR, Fluid Attenuated Inversion Recovery, simile alla T2 ma sopprime il segnale del liquido cefalorachidiano, che permette la diffusione di sostanze nutrienti dal sangue alle cellule nervose

Labels:
    0: senza etichetta
    1: nucleo tumorale necrotico, crescita incontrollata senza nutrienti (non si illumina col contrasto perchè non è vascolarizzata)
    2: edema peritumorale (accumulo di liquido causato da una infiammazione, intensa in t2 e flair, alterazione del tessuto cerebrale circostante)
    3: tumore attivo

STEP ONE:
    Unico step di preprocessing delle immagini
    Inizialmente i percorsi vengono caricati con glob, ogni campione è 240x240x155 e pesa circa 160 megabyte (di cui 17 per la mask)
    Per ogni "vista" del campione:
        Vengono fusi i primi due assi in modo che fit_transform possa normalizzare [0.0, 1.0] i valori della matrice (perchè lavora in 2D)
    Viene caricata la maschera in uint8 (più veloce, anche se il valore max è 4)
        Tutti i valori della matrice che hanno valore 4 vengono portati a valore 3 (per dare continuità)
            Questo perchè non ci sono pixel di classe 3 nella mask (è inutilizzata)
    Vengono "stackate" tutte le viste, ora ho un unico volume multicanale per il campione (240, 240, 155, 3)
    Eseguiamo un crop dei volumi in modo da avere multipli di 64 e da avere l'immagine centrata
        Il volume unico diventa (128, 128, 128, 3)
        Il volume della mask è (128, 128, 128)
    Se non ci sono abbastanza pixel interessanti nella mask (ci sono troppi pochi valori diversi da 0 (sfondo)) scarto il campione
    Altrimenti prendo la mask del campione e aggiungo un quarto canale tramite to_categorical
        Con to_categorical ho una codifica in stile one-hot:
            [0,1,2,1] --> [ [1,0,0], [0,1,0], [0,0,1], [0,1,0] ] 
                Se ho tre classi ogni valore viene espanso su tre canali e assume codifica one-hot
    Alla fine viene salvato un array numpy di 50 mb per il campione e 8 per la mask
    Con splitfolders creo il 75% di train e il 25% di val dal dataset di partenza, con le rispettive mask
    Alla fine ho la cartella ./input_data_128_split divisa e pronta
    Ho le immagini delle sezioni dei due volumi prima e dopo il crop

STEP TWO:
    Lo scopo è creare un DataLoader che possa caricare correttamente i dataset dividendoli in batch di 2 (default)
    Ho le immagini di T2, T1CE, FLAIR e la mask corrispondente da far vedere (griglia 2x2)
    Definisco il dataset che legge dai percorsi /images e /masks ogni file, li ordina
        BratsDataset è una classe che eredita da torch.utils.data.Dataset
    Definisco il DataLoader cercando di includere tutti i campioni (drop_last = False, comportamento di default)
        Il DataLoader è fondamentale per organizzare il BratsDataset in batch e per eseguire lo shuffling
    Alla fine dello step mi vengono ritornati i due loader

STEP THREE:
    Devo definire il modello, gli iperparametri e le metriche utilizzate
    Definisco inizialmente la DiceLoss come funzione di perdita
        Misura le somiglianze tra le maschere predette e quelle reali concentrandosi sulla sovrapposizione
        La inizializzo senza pesi in modo da valorizzare allo stesso modo ogni classe senza preferenze
        Cosa succede durante il forward?
            Prima c'è la Softmax così i logit del modello diventano probabilità di ogni classe
            Viene calcolato class_input (per ogni classe c'è un tensore di probabilità per ogni voxel)
                Ad esempio 2x2x2 contiene 32 informazioni (8x4 classi)
            Viene calcolato class_targets che contiene il numero di classe di ogni voxel 
                Ad esempio 2x2x2 contiene 8 target (classi dei voxel)
            Vengono calcolati unione e intersezione e viene ritornata la loss
                DICE coeff. = (2 * intersezione + smooth) / (union + smooth)
                DICE loss = 1 - DICE coeff.
                    Maggiore l'intersezione, minore la loss (idealmente 0)
    L'optimizer AdamW viene adottato automaticamente all'interno del metodo configure_optimizers del modello
    Devo definire la Unet 3D
        Blocco convoluzionale, Decoding Path ed Encoding Path
        ConvBlock:
            Kernel 3x3x3, padding = 'same', in modo che l'input abbia le stesse dimensioni dell'output
            Prima conv3d (in --> out), poi dropout (vengono spenti dei neuroni), poi conv3d (out --> out di nuovo)
                Dopo ogni conv3d è applicata la ReLU
        EncoderBlock:
            Usa un ConvBlock e poi fa il MaxPooling con kernel 2x2x2 e stride 2
            Ritorna il risultato del ConvBlock e il risultato del downsampling
            Estrae informazioni importanti dal volume e crea la feature map associata
        DecoderBlock:
            Usato per costruire la maschera di segmentazione finale
            Si utilizzano il ConvBlock e il convtranspose3d per l'upsampling, dove vengono raddoppiate le dimensioni
        Unet3D:
            Utilizza 4 EncoderBlock, un bottleneck, 4 DecoderBlock e una Conv3d finale che porta a 4 canali
            I pesi vengono inizializzati secondo il criterio Kaiming Normal
                Ottimizzato per i modelli che usano la ReLU
                Permette di limitare il problema del vanishing/exploding gradient
                    Il primo rallenta la convergenza, il secondo causa divergenza
            Vengono utilizzate le seguenti metriche:
                Accuracy semplice = predizioni corrette / predizioni totali
                Jaccard index = intersezione / unione
                    Di A (area delle predizioni) e B (area delle ground truth)
            Definisco poi il Training_Step:
                Per ogni batch calcolo le predizioni
                Posso stampare l'accuracy e l'indice di Jaccard
            Definisco poi il Validation_Step:
                Faccio lo stesso.
    
    STEP FINALE:
        Definisco il callback per il checkpoint del modello e il Trainer
        Inizia il fit
            
